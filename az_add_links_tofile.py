from os import path

import csv 
from datetime import datetime
import pandas as pd

from bs4 import BeautifulSoup
import requests
import json


# adds link to csv file generated by initial scraper
def add_links_az():
#     output_csv = 'az_raw.csv'
    max_entries = 550 # manually inserted
    # this scraper has to be checked in on periodically to make sure that the data entries are still below 550, otherwise we could be missing data.
    start_row_list = range(1, max_entries, 50)

    # Load for first time => get header
    start_row = 1

    links = []
    for start_row in start_row_list:
        try:
            url = 'https://www.azjobconnection.gov/ada/mn_warn_dsp.cfm?securitysys=on&start_row={}&max_rows=50&orderby=sda&choice=1'.format(start_row)
            page = requests.get(url)

            print(page.status_code) # should be 200

            soup = BeautifulSoup(page.text, 'html.parser')

            table = soup.find_all('table') # output is list-type
            for a in soup.find_all('a', href=True, text=True):
                link_text = a['href']
    #             print(link_text)
                if 'callingfile' in link_text:
                    links.append(link_text)

        except IndexError:
            print(url + ' not found')


    data = pd.read_csv('/Users/dilcia_mercedes/Big_Local_News/prog/WARN/data/arizona_warn_raw.csv')
    data['url_suffix'] = links
    data['Employer Name'] = data['Employer'].str.replace('\r', '')
    data.drop(columns='Employer', inplace=True)
    data = data[['url_suffix', 'Employer Name', 'City', 'Zip', 'LWIB Area', 'Notice Date']]
    # print(data)
    data.to_csv('/Users/dilcia_mercedes/Big_Local_News/prog/WARN/data/arizona_warn_raw.csv')

if __name__ == '__main__':
    add_links_az()


